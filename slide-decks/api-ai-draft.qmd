---
title: "שילוב בינה מלאכותית במחקר ארכיאולוגי"
subtitle: "מדריך להתקנה ושימוש ב-Hugging Face ו-Ollama"
author: "ד״ר ישראל ישראלי"
format: 
  revealjs:
    rtl: true
    slide-number: true
    theme: serif
    transition: slide
    background-transition: fade
    highlight-style: github
    code-fold: false
    code-tools: true
    code-line-numbers: true
editor: visual
---

# מבוא {.center}

## מדוע להשתמש בבינה מלאכותית במחקר ארכיאולוגי?

::: incremental
-   ניתוח טקסטים היסטוריים בהיקף נרחב
-   זיהוי דפוסים בנתונים ארכיאולוגיים
-   סיוע בתרגום ופענוח כתובות עתיקות
-   הצלבת מקורות ומציאת קשרים בין ממצאים
-   יצירת שחזורים והדמיות של אתרים עתיקים
:::

## מודלי בינה מלאכותית לשימוש החוקר

::::: columns
::: {.column width="50%"}
**Hugging Face**

-   מגוון מודלים מוכנים לשימוש
-   גישה באמצעות API מקוון
-   חינמי לשימוש בסיסי
-   מתאים למחקר אקדמי
:::

::: {.column width="50%"}
**Ollama**

-   הרצה מקומית על המחשב האישי
-   ללא צורך בחיבור לאינטרנט לאחר ההורדה
-   שליטה מלאה על המודלים
-   פרטיות מוחלטת של הנתונים
:::
:::::

# התקנת Hugging Face API {.center}

## צעד 1: יצירת חשבון ב-Hugging Face

::: incremental
1.  גשו לאתר [huggingface.co](https://huggingface.co/) והירשמו לחשבון חינמי
2.  נווטו אל הפרופיל שלכם ← הגדרות ← Access Tokens
3.  צרו טוקן חדש (בחרו ברמת הרשאות "Read" לגישה חינמית)
:::

``` {.bash filename="Terminal"}
# שמירת הטוקן כמשתנה סביבה
export HF_API_TOKEN="hf_your_token_here"
```

## צעד 2: התקנת חבילות נדרשות ב-R

``` {.r filename="R Console"}
# התקנת חבילות נדרשות
install.packages(c("httr", "jsonlite"))
library(httr)
library(jsonlite)

# הגדרת הטוקן כמשתנה סביבה
Sys.setenv(HF_API_TOKEN = "hf_your_token_here")
```

## צעד 3: יצירת פונקציה לגישה ל-API

``` {.r filename="R Function"}
hf_inference <- function(model_id, inputs, task = "text-generation", parameters = NULL) {
  API_URL <- paste0("https://api-inference.huggingface.co/models/", model_id)
  
  body <- list(inputs = inputs)
  if (!is.null(parameters)) {
    body$parameters <- parameters
  }
  
  response <- POST(
    url = API_URL,
    add_headers(
      Authorization = paste("Bearer", Sys.getenv("HF_API_TOKEN")),
      "Content-Type" = "application/json"
    ),
    body = toJSON(body, auto_unbox = TRUE)
  )
  
  content(response)
}
```

## צעד 4: שימוש במודלים ספציפיים - דוגמה ראשונה

``` {.r filename="Text Generation with Mistral"}
# יצירת טקסט באמצעות Mistral-7B-Instruct
result <- hf_inference(
  model_id = "mistralai/Mistral-7B-Instruct-v0.2",
  inputs = "<s>[INST] תאר את הממלכה האשורית החדשה בתקופת שלטונו של אשורבניפל [/INST]",
  parameters = list(
    max_new_tokens = 250,
    temperature = 0.7,
    return_full_text = FALSE
  )
)

# הדפסת התוצאה
cat(result[[1]]$generated_text)
```

## צעד 5: שימוש במודלים ספציפיים - דוגמה שנייה

``` {.r filename="Question Answering with BERT"}
result <- hf_inference(
  model_id = "deepset/roberta-base-squad2",
  inputs = list(
    question = "מה היה הישג מרכזי של אשורבניפל?",
    context = "אשורבניפל (668-627 לפנה״ס) היה המלך האחרון של האימפריה האשורית החדשה. הוא ידוע בהקמת הספרייה הגדולה בנינוה, שבה נאספו אלפי לוחות חרס ובהם טקסטים מסופוטמיים."
  )
)

# הדפסת התשובה
cat("תשובה:", result$answer)
```

# התקנת Ollama API {.center}

## צעד 1: התקנת Ollama

::: incremental
1.  גשו לאתר [ollama.ai](https://ollama.ai/) והורידו את הגרסה המתאימה למערכת ההפעלה שלכם
2.  **Windows**: הורידו והתקינו את התוכנה
3.  **macOS**: הורידו את האפליקציה או הריצו `curl -fsSL https://ollama.ai/install.sh | sh`
4.  **Linux**: הריצו `curl -fsSL https://ollama.ai/install.sh | sh`
:::

## צעד 2: הפעלת Ollama והורדת מודל

``` {.bash filename="Terminal"}
# הפעלת השירות
ollama serve

# הורדת מודל קל משקל להתחלה
ollama pull tinyllama

# הורדת מודל עם הקשר רחב יותר
ollama pull llama2
```

## צעד 3: יצירת פונקציות ב-R לשימוש ב-Ollama

``` {.r filename="R Functions"}
library(httr)
library(jsonlite)

ollama_generate <- function(prompt, model = "tinyllama", parameters = NULL) {
  # פרמטרים בסיסיים
  body <- list(
    model = model,
    prompt = prompt
  )
  
  # הוספת פרמטרים מותאמים אישית
  if (!is.null(parameters)) {
    body <- c(body, parameters)
  }
  
  response <- POST(
    url = "http://localhost:11434/api/generate",
    body = toJSON(body, auto_unbox = TRUE),
    encode = "json"
  )
  
  content(response)
}
```

## צעד 4: פונקציית צ׳אט לשיחות מורכבות

``` {.r filename="Chat Function"}
ollama_chat <- function(messages, model = "tinyllama") {
  response <- POST(
    url = "http://localhost:11434/api/chat",
    body = toJSON(list(
      model = model,
      messages = messages
    ), auto_unbox = TRUE),
    encode = "json"
  )
  
  content(response)
}
```

## צעד 5: שימוש בפונקציות

``` {.r filename="Using Ollama"}
# יצירת טקסט פשוטה
result <- ollama_generate(
  prompt = "תאר את חורבן ממלכת ישראל על ידי האשורים בשנת 722 לפנה״ס",
  model = "llama2"
)

# הצגת התוצאה
cat(result$response)

# לשיחה בסגנון צ׳אט
chat_result <- ollama_chat(
  messages = list(
    list(role = "user", content = "מה היו הסיבות העיקריות להתמוטטות הערים הכנעניות בסוף תקופת הברונזה המאוחרת?")
  ),
  model = "llama2"
)

cat(chat_result$message$content)
```

# חלונות הקשר (Context Windows) {.center}

## מהו חלון הקשר?

::: incremental
-   **חלון הקשר** הוא כמות המידע שהמודל יכול "לזכור" ולעבד בבת אחת
-   נמדד ב**טוקנים** (בערך 0.75 מילים בעברית)
-   משפיע מאוד על יכולת הניתוח של טקסטים ארוכים
-   ככל שחלון ההקשר גדול יותר, כך המודל "זוכר" יותר מידע
:::

## השוואה בין חלונות הקשר

::::: columns
::: {.column width="50%"}
**חלון הקשר רגיל (2K-4K טוקנים)**

-   מתאים למחשבים עם 8GB RAM
-   תגובות מהירות יותר
-   מספיק לשאלות קצרות
-   לניתוח טקסטים קצרים
:::

::: {.column width="50%"}
**חלון הקשר גדול (8K-32K+ טוקנים)**

-   דורש 16GB+ RAM
-   איטי יותר בהרצה
-   מתאים לניתוח מסמכים שלמים
-   זוכר פרטים משיחות ארוכות
:::
:::::

## מודלים לפי גודל חלון הקשר

::::: columns
::: {.column width="50%"}
**חלון הקשר רגיל**

-   `ollama pull tinyllama` (\~2K)
-   `ollama pull llama2` (\~4K)
-   `ollama pull mistral` (\~4K)
-   `ollama pull phi` (\~4K)
:::

::: {.column width="50%"}
**חלון הקשר גדול**

-   `ollama pull llama2-70b` (\~8K)
-   `ollama pull wizard:13b` (\~8K)
-   `ollama pull mixtral` (\~8K)
-   `ollama pull claude-instant` (\~16K)
:::
:::::

# יישומים בארכיאולוגיה של הלבנט {.center}

## תקופת הברונזה המאוחרת: מכתבי אל-עמארנה

::::: columns
::: {.column width="60%"}
**אתגר המחקר:**

-   382 לוחות חרס בכתב יתדות
-   מכתבים דיפלומטיים בין מצרים לכנען
-   מקור מידע מרכזי על הלבנט במאה ה-14 לפנה״ס
-   הבנת יחסי הכוחות בין ערי כנען

**מודל מומלץ:** מודל עם חלון הקשר גדול
:::

::: {.column width="40%"}
![מכתב מארכיון אל-עמארנה](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Amarna_Akkadian_letter.png/800px-Amarna_Akkadian_letter.png)
:::
:::::

## קוד לדוגמה: ניתוח מכתבי אל-עמארנה

``` {.r filename="Amarna Letters Analysis"}
# ניתוח ארכיון אל-עמארנה עם חלון הקשר גדול
amarna_analysis <- ollama_chat(
  messages = list(
    list(role = "user", content = "להלן תעתיק של מכתב EA 286 מארכיון אל-עמארנה, שנשלח על ידי עבדי-חבה מירושלים לפרעה. נתח את התוכן, הקשר ההיסטורי והמשמעות הפוליטית שלו:

[כאן יופיע תעתיק מלא של המכתב שאורכו כ-300 מילים]")
  ),
  model = "wizard:13b"  # מודל עם חלון הקשר גדול
)

cat(amarna_analysis$message$content)
```

## תקופת הברזל: כתובת מישע

::::: columns
::: {.column width="50%"}
**אתגר המחקר:**

-   כתובת מואבית מהמאה ה-9 לפנה״ס
-   34 שורות בכתב עברי קדום
-   מקור היסטורי מקביל לתנ״ך
-   משקפת את המתחים בין ישראל ומואב

**מודל מומלץ:** מודל עם חלון הקשר רגיל
:::

::: {.column width="50%"}
![כתובת מישע](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Stele_of_Mesha_01.jpg/800px-Stele_of_Mesha_01.jpg)
:::
:::::

## קוד לדוגמה: השוואת גרסאות של כתובת מישע

``` {.r filename="Mesha Stele Comparison"}
# השוואת פרשנויות שונות לכתובת מישע
mesha_interpretations <- hf_inference(
  model_id = "mistralai/Mistral-7B-Instruct-v0.2",
  inputs = "<s>[INST] השווה בין הפרשנויות של אנדרה למואר ומארק ליידנר לשורות 31-32 בכתובת מישע, בהקשר לאזכור 'בית דוד'. האם ניתן לראות בכתובת זו עדות חוץ-מקראית לקיומה של שושלת דוד? [/INST]",
  parameters = list(
    max_new_tokens = 500,
    temperature = 0.3
  )
)

cat(mesha_interpretations[[1]]$generated_text)
```

## מקורות אשוריים ובבליים: פריזמת סנחריב

::::: columns
::: {.column width="50%"}
**אתגר המחקר:**

-   טקסט ארוך על 6 צדדים של פריזמה
-   תיאור מסעות הכיבוש של סנחריב
-   כולל מידע על המצור על ירושלים
-   מקור היסטורי מקביל לתנ״ך

**מודל מומלץ:** מודל עם חלון הקשר גדול
:::

::: {.column width="50%"}
![פריזמת סנחריב](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Taylor_Prism%2C_British_Museum%2C_London.jpg/800px-Taylor_Prism%2C_British_Museum%2C_London.jpg)
:::
:::::

## קוד לדוגמה: ניתוח פריזמת סנחריב

``` {.r filename="Sennacherib Prism Analysis"}
# ניתוח מלא של פריזמת סנחריב (טקסט ארוך)
sennacherib_analysis <- ollama_chat(
  messages = list(
    list(role = "user", content = "להלן התרגום המלא של פריזמת טיילור (פריזמת סנחריב) המתארת את מסע המלחמה השלישי של סנחריב לארץ ישראל בשנת 701 לפנה״ס. נתח את הטקסט המלא, השווה אותו למקורות המקראיים במלכים ב, ישעיהו ודברי הימים, והסבר את המשמעות ההיסטורית והארכיאולוגית:

[כאן יופיע התרגום המלא של הפריזמה, שאורכו כ-1000 מילים]"),
  ),
  model = "claude-instant"  # מודל עם חלון הקשר גדול במיוחד
)

cat(sennacherib_analysis$message$content)
```

## תעודות בבליות: יומני אסטרונומיה

::::: columns
::: {.column width="50%"}
**אתגר המחקר:**

-   יומנים אסטרונומיים מבבל
-   רישום תופעות שמיימיות לאורך 700 שנה
-   סיוע בתיארוך אירועים היסטוריים
-   מאפשר לייצר כרונולוגיה אבסולוטית

**מודל מומלץ:** מודל עם חלון הקשר בינוני
:::

::: {.column width="50%"}
![יומן אסטרונומי בבלי](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/British_Museum_Venus_Tablet_of_Ammisaduqa.jpg/800px-British_Museum_Venus_Tablet_of_Ammisaduqa.jpg)
:::
:::::

## קוד לדוגמה: תיארוך ליקוי חמה

``` {.r filename="Babylonian Astronomical Diaries"}
# ניתוח יומן אסטרונומי ותיארוך אירוע היסטורי
astronomical_dating <- hf_inference(
  model_id = "mistralai/Mistral-7B-Instruct-v0.2",
  inputs = "<s>[INST] לפי היומן האסטרונומי הבבלי BM 32312, ליקוי חמה נצפה בחודש סיון בשנה ה-7 למלך נבוכדנאצר השני. בהתבסס על חישובים אסטרונומיים מודרניים, מתי התרחש ליקוי זה בלוח השנה הגרגוריאני? מה המשמעות של תאריך זה לכרונולוגיה של חורבן ירושלים? [/INST]",
  parameters = list(
    max_new_tokens = 400,
    temperature = 0.2
  )
)

cat(astronomical_dating[[1]]$generated_text)
```

# סיכום והמלצות {.center}

## בחירת המודל המתאים למחקר

::: incremental
-   **טקסטים קצרים וממוקדים** - מודל עם חלון הקשר רגיל:
    -   כתובות בודדות (מישע, שילוח, כתובות קבורה)
    -   ניתוח פריטים ארכיאולוגיים ספציפיים
    -   תיארוך וקטלוג ממצאים
-   **טקסטים ארוכים ומורכבים** - מודל עם חלון הקשר גדול:
    -   ארכיונים שלמים (אל-עמארנה, לכיש)
    -   אנאלים מלכותיים אשוריים ובבליים
    -   השוואה בין מקורות היסטוריים מרובים
:::

## המלצות מעשיות לשימוש

1.  **התחילו עם Hugging Face** לניסויים ראשוניים
2.  **התקינו Ollama** עבור מחקר מתמשך ועמוק
3.  **בחרו את גודל החלון** בהתאם לטקסט הנחקר
4.  **שמרו על גישה ביקורתית** - המודלים אינם מושלמים
5.  **תעדו את השאילתות והתוצאות** כחלק ממתודולוגיית המחקר

## תודה רבה! {.center}

**שאלות?**

------------------------------------------------------------------------

## מקורות וקישורים שימושיים

-   [Hugging Face - אתר רשמי](https://huggingface.co/)
-   [Ollama - אתר רשמי](https://ollama.ai/)
-   [מדריך למודלים שפתיים בארכיאולוגיה](https://medium.com/@archaeoai/large-language-models-in-archaeology-a-beginners-guide-7da2f4e3db1b)
-   [פרויקט DigitalPasts - כלי בינה מלאכותית בארכיאולוגיה](https://digitalpasts.github.io/)
